
@misc{zhang_k-net_2021,
	title = {K-Net: Towards Unified Image Segmentation},
	url = {http://arxiv.org/abs/2106.14855},
	doi = {10.48550/arXiv.2106.14855},
	shorttitle = {K-Net},
	abstract = {Semantic, instance, and panoptic segmentations have been addressed using different and specialized frameworks despite their underlying connections. This paper presents a unified, simple, and effective framework for these essentially similar tasks. The framework, named K-Net, segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. To remedy the difficulties of distinguishing various instances, we propose a kernel update strategy that enables each kernel dynamic and conditional on its meaningful group in the input image. K-Net can be trained in an end-to-end manner with bipartite matching, and its training and inference are naturally {NMS}-free and box-free. Without bells and whistles, K-Net surpasses all previous published state-of-the-art single-model results of panoptic segmentation on {MS} {COCO} test-dev split and semantic segmentation on {ADE}20K val split with 55.2\% {PQ} and 54.3\% {mIoU}, respectively. Its instance segmentation performance is also on par with Cascade Mask R-{CNN} on {MS} {COCO} with 60\%-90\% faster inference speeds. Code and models will be released at https://github.com/{ZwwWayne}/K-Net/.},
	number = {{arXiv}:2106.14855},
	publisher = {{arXiv}},
	author = {Zhang, Wenwei and Pang, Jiangmiao and Chen, Kai and Loy, Chen Change},
	urldate = {2023-03-17},
	date = {2021-11-01},
	eprinttype = {arxiv},
	eprint = {2106.14855 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/4IULU2GF/2106.html:text/html;Zhang et al_2021_K-Net.pdf:/home/hugo/Zotero/storage/GDB386SM/Zhang et al_2021_K-Net.pdf:application/pdf},
}

@misc{cheng_masked-attention_2022,
	title = {Masked-attention Mask Transformer for Universal Image Segmentation},
	url = {http://arxiv.org/abs/2112.01527},
	doi = {10.48550/arXiv.2112.01527},
	abstract = {Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 {PQ} on {COCO}), instance segmentation (50.1 {AP} on {COCO}) and semantic segmentation (57.7 {mIoU} on {ADE}20K).},
	number = {{arXiv}:2112.01527},
	publisher = {{arXiv}},
	author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
	urldate = {2023-03-17},
	date = {2022-06-15},
	eprinttype = {arxiv},
	eprint = {2112.01527 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/BKUATXBQ/2112.html:text/html;Cheng et al_2022_Masked-attention Mask Transformer for Universal Image Segmentation.pdf:/home/hugo/Zotero/storage/BHM8MP2H/Cheng et al_2022_Masked-attention Mask Transformer for Universal Image Segmentation.pdf:application/pdf},
}

@misc{csurka_semantic_2023,
	title = {Semantic Image Segmentation: Two Decades of Research},
	url = {http://arxiv.org/abs/2302.06378},
	doi = {10.48550/arXiv.2302.06378},
	shorttitle = {Semantic Image Segmentation},
	abstract = {Semantic image segmentation ({SiS}) plays a fundamental role in a broad variety of computer vision applications, providing key information for the global understanding of an image. This survey is an effort to summarize two decades of research in the field of {SiS}, where we propose a literature review of solutions starting from early historical methods followed by an overview of more recent deep learning methods including the latest trend of using transformers. We complement the review by discussing particular cases of the weak supervision and side machine learning techniques that can be used to improve the semantic segmentation such as curriculum, incremental or self-supervised learning. State-of-the-art {SiS} models rely on a large amount of annotated samples, which are more expensive to obtain than labels for tasks such as image classification. Since unlabeled data is instead significantly cheaper to obtain, it is not surprising that Unsupervised Domain Adaptation ({UDA}) reached a broad success within the semantic segmentation community. Therefore, a second core contribution of this book is to summarize five years of a rapidly growing field, Domain Adaptation for Semantic Image Segmentation ({DASiS}) which embraces the importance of semantic segmentation itself and a critical need of adapting segmentation models to new environments. In addition to providing a comprehensive survey on {DASiS} techniques, we unveil also newer trends such as multi-domain learning, domain generalization, domain incremental learning, test-time adaptation and source-free domain adaptation. Finally, we conclude this survey by describing datasets and benchmarks most widely used in {SiS} and {DASiS} and briefly discuss related tasks such as instance and panoptic image segmentation, as well as applications such as medical image segmentation.},
	number = {{arXiv}:2302.06378},
	publisher = {{arXiv}},
	author = {Csurka, Gabriela and Volpi, Riccardo and Chidlovskii, Boris},
	urldate = {2023-03-17},
	date = {2023-02-13},
	eprinttype = {arxiv},
	eprint = {2302.06378 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/HUZSH94W/2302.html:text/html;Csurka et al_2023_Semantic Image Segmentation.pdf:/home/hugo/Zotero/storage/A6TU6UGV/Csurka et al_2023_Semantic Image Segmentation.pdf:application/pdf},
}

@misc{geng_is_2021,
	title = {Is Attention Better Than Matrix Decomposition?},
	url = {http://arxiv.org/abs/2109.04553},
	doi = {10.48550/arXiv.2109.04553},
	abstract = {As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition ({MD}) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank recovery problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving {MDs} to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different {MDs} can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through {MDs}. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants.},
	number = {{arXiv}:2109.04553},
	publisher = {{arXiv}},
	author = {Geng, Zhengyang and Guo, Meng-Hao and Chen, Hongxu and Li, Xia and Wei, Ke and Lin, Zhouchen},
	urldate = {2023-03-17},
	date = {2021-12-28},
	eprinttype = {arxiv},
	eprint = {2109.04553 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/YPP6V88S/2109.html:text/html;Geng et al_2021_Is Attention Better Than Matrix Decomposition.pdf:/home/hugo/Zotero/storage/Y4GIKEN2/Geng et al_2021_Is Attention Better Than Matrix Decomposition.pdf:application/pdf},
}

@misc{yang_focal_2022,
	title = {Focal Modulation Networks},
	url = {http://arxiv.org/abs/2203.11926},
	doi = {10.48550/arXiv.2203.11926},
	abstract = {We propose focal modulation networks ({FocalNets} in short), where self-attention ({SA}) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show {FocalNets} outperform the state-of-the-art {SA} counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, {FocalNets} with tiny and base size achieve 82.3\% and 83.9\% top-1 accuracy on {ImageNet}-1K. After pretrained on {ImageNet}-22K in 224 resolution, it attains 86.5\% and 87.3\% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, {FocalNets} exhibit clear superiority. For object detection with Mask R-{CNN}, {FocalNet} base trained with 1{\textbackslash}times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3{\textbackslash}times schedule (49.0 v.s. 48.5). For semantic segmentation with {UPerNet}, {FocalNet} base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large {FocalNet} and Mask2former, we achieve 58.5 {mIoU} for {ADE}20K semantic segmentation, and 57.9 {PQ} for {COCO} Panoptic Segmentation. Using huge {FocalNet} and {DINO}, we achieved 64.3 and 64.4 {mAP} on {COCO} minival and test-dev, respectively, establishing new {SoTA} on top of much larger attention-based models like Swinv2-G and {BEIT}-3. Code and checkpoints are available at https://github.com/microsoft/{FocalNet}.},
	number = {{arXiv}:2203.11926},
	publisher = {{arXiv}},
	author = {Yang, Jianwei and Li, Chunyuan and Dai, Xiyang and Yuan, Lu and Gao, Jianfeng},
	urldate = {2023-03-16},
	date = {2022-11-05},
	eprinttype = {arxiv},
	eprint = {2203.11926 [cs]},
	note = {version: 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Yang et al_2022_Focal Modulation Networks.pdf:/home/hugo/Zotero/storage/JPLA4HNW/Yang et al_2022_Focal Modulation Networks.pdf:application/pdf},
}

@misc{cheng_per-pixel_2021,
	title = {Per-Pixel Classification is Not All You Need for Semantic Segmentation},
	url = {http://arxiv.org/abs/2107.06278},
	doi = {10.48550/arXiv.2107.06278},
	abstract = {Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose {MaskFormer}, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that {MaskFormer} outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 {mIoU} on {ADE}20K) and panoptic segmentation (52.7 {PQ} on {COCO}) models.},
	number = {{arXiv}:2107.06278},
	publisher = {{arXiv}},
	author = {Cheng, Bowen and Schwing, Alexander G. and Kirillov, Alexander},
	urldate = {2023-03-14},
	date = {2021-10-31},
	eprinttype = {arxiv},
	eprint = {2107.06278 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/MXNTBSFS/2107.html:text/html;Cheng et al_2021_Per-Pixel Classification is Not All You Need for Semantic Segmentation.pdf:/home/hugo/Zotero/storage/2D99VBAC/Cheng et al_2021_Per-Pixel Classification is Not All You Need for Semantic Segmentation.pdf:application/pdf},
}

@misc{liu_swin_2021,
	title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
	url = {http://arxiv.org/abs/2103.14030},
	doi = {10.48550/arXiv.2103.14030},
	shorttitle = {Swin Transformer},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on {ImageNet}-1K) and dense prediction tasks such as object detection (58.7 box {AP} and 51.1 mask {AP} on {COCO} test-dev) and semantic segmentation (53.5 {mIoU} on {ADE}20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box {AP} and +2.6 mask {AP} on {COCO}, and +3.2 {mIoU} on {ADE}20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-{MLP} architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	number = {{arXiv}:2103.14030},
	publisher = {{arXiv}},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	urldate = {2023-03-14},
	date = {2021-08-17},
	eprinttype = {arxiv},
	eprint = {2103.14030 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/DQTI6883/2103.html:text/html;Liu et al_2021_Swin Transformer.pdf:/home/hugo/Zotero/storage/QT7HTILX/Liu et al_2021_Swin Transformer.pdf:application/pdf},
}

@misc{chen_vision_2023,
	title = {Vision Transformer Adapter for Dense Predictions},
	url = {http://arxiv.org/abs/2205.08534},
	doi = {10.48550/arXiv.2205.08534},
	abstract = {This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer ({ViT}). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain {ViT} suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the {ViT}-Adapter, which allows plain {ViT} to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain {ViT} that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify {ViT}-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our {ViT}-Adapter-L yields state-of-the-art 60.9 box {AP} and 53.0 mask {AP} on {COCO} test-dev. We hope that the {ViT}-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. The code and models will be released at https://github.com/czczup/{ViT}-Adapter.},
	number = {{arXiv}:2205.08534},
	publisher = {{arXiv}},
	author = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
	urldate = {2023-03-14},
	date = {2023-02-13},
	eprinttype = {arxiv},
	eprint = {2205.08534 [cs]},
	note = {version: 4},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/LL4U3VXK/2205.html:text/html;Chen et al_2023_Vision Transformer Adapter for Dense Predictions.pdf:/home/hugo/Zotero/storage/69S8TDZM/Chen et al_2023_Vision Transformer Adapter for Dense Predictions.pdf:application/pdf},
}

@misc{chen_deeplab_2017,
	title = {{DeepLab}: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected {CRFs}},
	url = {http://arxiv.org/abs/1606.00915},
	doi = {10.48550/arXiv.1606.00915},
	shorttitle = {{DeepLab}},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling ({ASPP}) to robustly segment objects at multiple scales. {ASPP} probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from {DCNNs} and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in {DCNNs} achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final {DCNN} layer with a fully connected Conditional Random Field ({CRF}), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "{DeepLab}" system sets the new state-of-art at the {PASCAL} {VOC}-2012 semantic image segmentation task, reaching 79.7\% {mIOU} in the test set, and advances the results on three other datasets: {PASCAL}-Context, {PASCAL}-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	number = {{arXiv}:1606.00915},
	publisher = {{arXiv}},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	urldate = {2023-03-29},
	date = {2017-05-11},
	eprinttype = {arxiv},
	eprint = {1606.00915 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/X2UU4AEI/Chen et al. - 2017 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/2LNW2ZKF/1606.html:text/html},
}

@misc{li_system_2020,
	title = {A System for Massively Parallel Hyperparameter Tuning},
	url = {http://arxiv.org/abs/1810.05934},
	doi = {10.48550/arXiv.1810.05934},
	abstract = {Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called {ASHA}, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that {ASHA} outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating {ASHA} in Determined {AI}'s end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.},
	number = {{arXiv}:1810.05934},
	publisher = {{arXiv}},
	author = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
	urldate = {2023-08-18},
	date = {2020-03-15},
	eprinttype = {arxiv},
	eprint = {1810.05934 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/Y3EEKTG9/1810.html:text/html;Li et al_2020_A System for Massively Parallel Hyperparameter Tuning.pdf:/home/hugo/Zotero/storage/I9BXBZA3/Li et al_2020_A System for Massively Parallel Hyperparameter Tuning.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-08-15},
	date = {2023-08-01},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/N8AGXUV2/1706.html:text/html;Vaswani et al_2023_Attention Is All You Need.pdf:/home/hugo/Zotero/storage/WSN3D3W8/Vaswani et al_2023_Attention Is All You Need.pdf:application/pdf},
}

@misc{chen_encoder-decoder_2018,
	title = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
	url = {http://arxiv.org/abs/1802.02611},
	doi = {10.48550/arXiv.1802.02611},
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, {DeepLabv}3+, extends {DeepLabv}3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on {PASCAL} {VOC} 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
	number = {{arXiv}:1802.02611},
	publisher = {{arXiv}},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	urldate = {2023-05-25},
	date = {2018-08-22},
	eprinttype = {arxiv},
	eprint = {1802.02611 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/4FNCDK5N/1802.html:text/html;Chen et al_2018_Encoder-Decoder with Atrous Separable Convolution for Semantic Image.pdf:/home/hugo/Zotero/storage/G3XRSAPE/Chen et al_2018_Encoder-Decoder with Atrous Separable Convolution for Semantic Image.pdf:application/pdf},
}

@misc{hoffer_train_2018,
	title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	url = {http://arxiv.org/abs/1705.08741},
	doi = {10.48550/arXiv.1705.08741},
	shorttitle = {Train longer, generalize better},
	abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on {MNIST}, {CIFAR}-10, {CIFAR}-100 and {ImageNet}. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
	number = {{arXiv}:1705.08741},
	publisher = {{arXiv}},
	author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	urldate = {2023-05-17},
	date = {2018-01-01},
	eprinttype = {arxiv},
	eprint = {1705.08741 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/8U7GJ45K/1705.html:text/html;Hoffer et al_2018_Train longer, generalize better.pdf:/home/hugo/Zotero/storage/8YULNTCI/Hoffer et al_2018_Train longer, generalize better.pdf:application/pdf},
}

@misc{yu_metaformer_2022,
	title = {{MetaFormer} Is Actually What You Need for Vision},
	url = {http://arxiv.org/abs/2111.11418},
	doi = {10.48550/arXiv.2111.11418},
	abstract = {Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in Transformers can be replaced by spatial {MLPs} and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the Transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in Transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as {PoolFormer}, achieves competitive performance on multiple computer vision tasks. For example, on {ImageNet}-1K, {PoolFormer} achieves 82.1\% top-1 accuracy, surpassing well-tuned Vision Transformer/{MLP}-like baselines {DeiT}-B/{ResMLP}-B24 by 0.3\%/1.1\% accuracy with 35\%/52\% fewer parameters and 50\%/62\% fewer {MACs}. The effectiveness of {PoolFormer} verifies our hypothesis and urges us to initiate the concept of "{MetaFormer}", a general architecture abstracted from Transformers without specifying the token mixer. Based on the extensive experiments, we argue that {MetaFormer} is the key player in achieving superior results for recent Transformer and {MLP}-like models on vision tasks. This work calls for more future research dedicated to improving {MetaFormer} instead of focusing on the token mixer modules. Additionally, our proposed {PoolFormer} could serve as a starting baseline for future {MetaFormer} architecture design. Code is available at https://github.com/sail-sg/poolformer.},
	number = {{arXiv}:2111.11418},
	publisher = {{arXiv}},
	author = {Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
	urldate = {2023-04-17},
	date = {2022-07-04},
	eprinttype = {arxiv},
	eprint = {2111.11418 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/HG889LTB/2111.html:text/html;Yu et al_2022_MetaFormer Is Actually What You Need for Vision.pdf:/home/hugo/Zotero/storage/VCEL5PMW/Yu et al_2022_MetaFormer Is Actually What You Need for Vision.pdf:application/pdf},
}

@misc{kirillov_segment_2023,
	title = {Segment Anything},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything ({SA}) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model ({SAM}) and corresponding dataset ({SA}-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	number = {{arXiv}:2304.02643},
	publisher = {{arXiv}},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	urldate = {2023-04-06},
	date = {2023-04-05},
	eprinttype = {arxiv},
	eprint = {2304.02643 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/W8D6KSNL/2304.html:text/html;Kirillov et al_2023_Segment Anything.pdf:/home/hugo/Zotero/storage/B32JRDJ7/Kirillov et al_2023_Segment Anything.pdf:application/pdf},
}

@misc{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	number = {{arXiv}:1505.04597},
	publisher = {{arXiv}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2023-08-19},
	date = {2015-05-18},
	eprinttype = {arxiv},
	eprint = {1505.04597 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/UNH8YFD7/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/ZI5K3IIA/1505.html:text/html},
}

@misc{zhao_pyramid_2017,
	title = {Pyramid Scene Parsing Network},
	url = {http://arxiv.org/abs/1612.01105},
	doi = {10.48550/arXiv.1612.01105},
	abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network ({PSPNet}). Our global prior representation is effective to produce good quality results on the scene parsing task, while {PSPNet} provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in {ImageNet} scene parsing challenge 2016, {PASCAL} {VOC} 2012 benchmark and Cityscapes benchmark. A single {PSPNet} yields new record of {mIoU} accuracy 85.4\% on {PASCAL} {VOC} 2012 and accuracy 80.2\% on Cityscapes.},
	number = {{arXiv}:1612.01105},
	publisher = {{arXiv}},
	author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
	urldate = {2023-08-19},
	date = {2017-04-27},
	eprinttype = {arxiv},
	eprint = {1612.01105 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/7CJZDFNR/Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/QQQWRDFK/1612.html:text/html},
}

@misc{chen_semantic_2016,
	title = {Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected {CRFs}},
	url = {http://arxiv.org/abs/1412.7062},
	doi = {10.48550/arXiv.1412.7062},
	abstract = {Deep Convolutional Neural Networks ({DCNNs}) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from {DCNNs} and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of {DCNNs} are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make {DCNNs} good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final {DCNN} layer with a fully connected Conditional Random Field ({CRF}). Qualitatively, our "{DeepLab}" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the {PASCAL} {VOC}-2012 semantic image segmentation task, reaching 71.6\% {IOU} accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern {GPU}.},
	number = {{arXiv}:1412.7062},
	publisher = {{arXiv}},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	urldate = {2023-08-19},
	date = {2016-06-07},
	eprinttype = {arxiv},
	eprint = {1412.7062 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/SQKWQ6YB/Chen et al. - 2016 - Semantic Image Segmentation with Deep Convolutiona.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/IFWUMNNU/1412.html:text/html},
}

@misc{chen_rethinking_2017,
	title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
	url = {http://arxiv.org/abs/1706.05587},
	doi = {10.48550/arXiv.1706.05587},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `{DeepLabv}3' system significantly improves over our previous {DeepLab} versions without {DenseCRF} post-processing and attains comparable performance with other state-of-art models on the {PASCAL} {VOC} 2012 semantic image segmentation benchmark.},
	number = {{arXiv}:1706.05587},
	publisher = {{arXiv}},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	urldate = {2023-08-19},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.05587 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/Y89FWSTU/Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image S.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/4255ICIW/1706.html:text/html},
}

@misc{xiao_unified_2018,
	title = {Unified Perceptual Parsing for Scene Understanding},
	url = {http://arxiv.org/abs/1807.10221},
	doi = {10.48550/arXiv.1807.10221},
	abstract = {Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called {UPerNet} and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes. Models are available at {\textbackslash}url\{https://github.com/{CSAILVision}/unifiedparsing\}.},
	number = {{arXiv}:1807.10221},
	publisher = {{arXiv}},
	author = {Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
	urldate = {2023-08-19},
	date = {2018-07-26},
	eprinttype = {arxiv},
	eprint = {1807.10221 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/C5ECXXIF/Xiao et al. - 2018 - Unified Perceptual Parsing for Scene Understanding.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/WQ5V868J/1807.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	number = {{arXiv}:2010.11929},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2023-08-19},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2010.11929 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/758THC3H/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/CKH8RWSD/2010.html:text/html},
}

@misc{strudel_segmenter_2021,
	title = {Segmenter: Transformer for Semantic Segmentation},
	url = {http://arxiv.org/abs/2105.05633},
	doi = {10.48550/arXiv.2105.05633},
	shorttitle = {Segmenter},
	abstract = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer ({ViT}) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both {ADE}20K and Pascal Context datasets and is competitive on Cityscapes.},
	number = {{arXiv}:2105.05633},
	publisher = {{arXiv}},
	author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
	urldate = {2023-08-20},
	date = {2021-09-02},
	eprinttype = {arxiv},
	eprint = {2105.05633 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/7NBTT6JS/Strudel et al. - 2021 - Segmenter Transformer for Semantic Segmentation.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/925DFI8A/2105.html:text/html},
}

@misc{xie_segformer_2021,
	title = {{SegFormer}: Simple and Efficient Design for Semantic Segmentation with Transformers},
	url = {http://arxiv.org/abs/2105.15203},
	doi = {10.48550/arXiv.2105.15203},
	shorttitle = {{SegFormer}},
	abstract = {We present {SegFormer}, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception ({MLP}) decoders. {SegFormer} has two appealing features: 1) {SegFormer} comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) {SegFormer} avoids complex decoders. The proposed {MLP} decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from {SegFormer}-B0 to {SegFormer}-B5, reaching significantly better performance and efficiency than previous counterparts. For example, {SegFormer}-B4 achieves 50.3\% {mIoU} on {ADE}20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, {SegFormer}-B5, achieves 84.0\% {mIoU} on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/{NVlabs}/{SegFormer}.},
	number = {{arXiv}:2105.15203},
	publisher = {{arXiv}},
	author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
	urldate = {2023-08-20},
	date = {2021-10-28},
	eprinttype = {arxiv},
	eprint = {2105.15203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/YPEDDEHQ/Xie et al. - 2021 - SegFormer Simple and Efficient Design for Semanti.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/R4Y658ME/2105.html:text/html},
}

@misc{long_fully_2015,
	title = {Fully Convolutional Networks for Semantic Segmentation},
	url = {http://arxiv.org/abs/1411.4038},
	doi = {10.48550/arXiv.1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks ({AlexNet}, the {VGG} net, and {GoogLeNet}) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of {PASCAL} {VOC} (20\% relative improvement to 62.2\% mean {IU} on 2012), {NYUDv}2, and {SIFT} Flow, while inference takes one third of a second for a typical image.},
	number = {{arXiv}:1411.4038},
	publisher = {{arXiv}},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	urldate = {2023-08-20},
	date = {2015-03-08},
	eprinttype = {arxiv},
	eprint = {1411.4038 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/hugo/Zotero/storage/MLKHFI6C/1411.html:text/html;Long et al_2015_Fully Convolutional Networks for Semantic Segmentation.pdf:/home/hugo/Zotero/storage/Q2GNFUYY/Long et al_2015_Fully Convolutional Networks for Semantic Segmentation.pdf:application/pdf},
}

@misc{zhou_semantic_2018,
	title = {Semantic Understanding of Scenes through the {ADE}20K Dataset},
	url = {http://arxiv.org/abs/1608.05442},
	doi = {10.48550/arXiv.1608.05442},
	abstract = {Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the {ADE}20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A generic network design called Cascade Segmentation Module is then proposed to enable the segmentation networks to parse a scene into stuff, objects, and object parts in a cascade. We evaluate the proposed module integrated within two existing semantic segmentation networks, yielding significant improvements for scene parsing. We further show that the scene parsing networks trained on {ADE}20K can be applied to a wide variety of scenes and objects.},
	number = {{arXiv}:1608.05442},
	publisher = {{arXiv}},
	author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
	urldate = {2023-08-20},
	date = {2018-10-16},
	eprinttype = {arxiv},
	eprint = {1608.05442 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/27F46C7E/Zhou et al. - 2018 - Semantic Understanding of Scenes through the ADE20.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/9QTW7XWP/1608.html:text/html},
}

@inproceedings{mottaghi_role_2014,
	title = {The Role of Context for Object Detection and Semantic Segmentation in the Wild},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Mottaghi_The_Role_of_2014_CVPR_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {891--898},
	author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
	urldate = {2023-08-20},
	date = {2014},
	file = {Full Text PDF:/home/hugo/Zotero/storage/WVFGPB2F/Mottaghi et al. - 2014 - The Role of Context for Object Detection and Seman.pdf:application/pdf},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: A large-scale hierarchical image database},
	doi = {10.1109/CVPR.2009.5206848},
	shorttitle = {{ImageNet}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “{ImageNet}”, a large-scale ontology of images built upon the backbone of the {WordNet} structure. {ImageNet} aims to populate the majority of the 80,000 synsets of {WordNet} with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of {WordNet}. This paper offers a detailed analysis of {ImageNet} in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that {ImageNet} is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of {ImageNet} through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of {ImageNet} can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	eventtitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {248--255},
	booktitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	date = {2009-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	file = {IEEE Xplore Abstract Record:/home/hugo/Zotero/storage/9T9JU5SG/5206848.html:text/html;IEEE Xplore Full Text PDF:/home/hugo/Zotero/storage/KVTJS8CN/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf},
}

@misc{bahdanau_neural_2016,
	title = {Neural Machine Translation by Jointly Learning to Align and Translate},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.48550/arXiv.1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	number = {{arXiv}:1409.0473},
	publisher = {{arXiv}},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	urldate = {2023-08-21},
	date = {2016-05-19},
	eprinttype = {arxiv},
	eprint = {1409.0473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/MX2VJ2ZA/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/3EY5NMUS/1409.html:text/html},
}

@misc{kim_structured_2017,
	title = {Structured Attention Networks},
	url = {http://arxiv.org/abs/1702.00887},
	doi = {10.48550/arXiv.1702.00887},
	abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
	number = {{arXiv}:1702.00887},
	publisher = {{arXiv}},
	author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
	urldate = {2023-08-21},
	date = {2017-02-16},
	eprinttype = {arxiv},
	eprint = {1702.00887 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/DZPYPGJR/Kim et al. - 2017 - Structured Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/E8RGKNNY/1702.html:text/html},
}

@misc{parikh_decomposable_2016,
	title = {A Decomposable Attention Model for Natural Language Inference},
	url = {http://arxiv.org/abs/1606.01933},
	doi = {10.48550/arXiv.1606.01933},
	abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference ({SNLI}) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
	number = {{arXiv}:1606.01933},
	publisher = {{arXiv}},
	author = {Parikh, Ankur P. and Täckström, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
	urldate = {2023-08-21},
	date = {2016-09-25},
	eprinttype = {arxiv},
	eprint = {1606.01933 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/T82K7HVD/Parikh et al. - 2016 - A Decomposable Attention Model for Natural Languag.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/D2UATWPB/1606.html:text/html},
}

@misc{luccioni_estimating_2022,
	title = {Estimating the Carbon Footprint of {BLOOM}, a 176B Parameter Language Model},
	url = {http://arxiv.org/abs/2211.02001},
	doi = {10.48550/arXiv.2211.02001},
	abstract = {Progress in machine learning ({ML}) comes with a cost to the environment, given that training {ML} models requires significant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of {BLOOM}, a 176-billion parameter language model, across its life cycle. We estimate that {BLOOM}'s final training emitted approximately 24.7 tonnes of{\textasciitilde}{\textbackslash}carboneq{\textasciitilde}if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an {API} endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of {ML} models and future research directions that can contribute towards improving carbon emissions reporting.},
	number = {{arXiv}:2211.02001},
	publisher = {{arXiv}},
	author = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
	urldate = {2023-08-21},
	date = {2022-11-03},
	eprinttype = {arxiv},
	eprint = {2211.02001 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/ETMHLFPT/Luccioni et al. - 2022 - Estimating the Carbon Footprint of BLOOM, a 176B P.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/YBMMT4DX/2211.html:text/html},
}

@misc{dai_coatnet_2021,
	title = {{CoAtNet}: Marrying Convolution and Attention for All Data Sizes},
	url = {http://arxiv.org/abs/2106.04803},
	doi = {10.48550/arXiv.2106.04803},
	shorttitle = {{CoAtNet}},
	abstract = {Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present {CoAtNets}(pronounced "coat" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our {CoAtNets} achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, {CoAtNet} achieves 86.0\% {ImageNet} top-1 accuracy; When pre-trained with 13M images from {ImageNet}-21K, our {CoAtNet} achieves 88.56\% top-1 accuracy, matching {ViT}-huge pre-trained with 300M images from {JFT}-300M while using 23x less data; Notably, when we further scale up {CoAtNet} with {JFT}-3B, it achieves 90.88\% top-1 accuracy on {ImageNet}, establishing a new state-of-the-art result.},
	number = {{arXiv}:2106.04803},
	publisher = {{arXiv}},
	author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V. and Tan, Mingxing},
	urldate = {2023-08-25},
	date = {2021-09-15},
	eprinttype = {arxiv},
	eprint = {2106.04803 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/NU8GKP8C/Dai et al. - 2021 - CoAtNet Marrying Convolution and Attention for Al.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/4VITHFET/2106.html:text/html},
}

@misc{li_can_2021,
	title = {Can Vision Transformers Perform Convolution?},
	url = {http://arxiv.org/abs/2111.01353},
	doi = {10.48550/arXiv.2111.01353},
	abstract = {Several recent studies have demonstrated that attention-based networks, such as Vision Transformer ({ViT}), can outperform Convolutional Neural Networks ({CNNs}) on several computer vision tasks without using convolutional layers. This naturally leads to the following questions: Can a self-attention layer of {ViT} express any convolution operation? In this work, we prove that a single {ViT} layer with image patches as the input can perform any convolution operation constructively, where the multi-head attention mechanism and the relative positional encoding play essential roles. We further provide a lower bound on the number of heads for Vision Transformers to express {CNNs}. Corresponding with our analysis, experimental results show that the construction in our proof can help inject convolutional bias into Transformers and significantly improve the performance of {ViT} in low data regimes.},
	number = {{arXiv}:2111.01353},
	publisher = {{arXiv}},
	author = {Li, Shanda and Chen, Xiangning and He, Di and Hsieh, Cho-Jui},
	urldate = {2023-08-25},
	date = {2021-11-02},
	eprinttype = {arxiv},
	eprint = {2111.01353 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/S2BPUWVZ/Li et al. - 2021 - Can Vision Transformers Perform Convolution.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/CNJIXUYP/2111.html:text/html},
}

@misc{heffels_aerial_2020,
	title = {Aerial Imagery Pixel-level Segmentation},
	url = {http://arxiv.org/abs/2012.02024},
	doi = {10.48550/arXiv.2012.02024},
	abstract = {Aerial imagery can be used for important work on a global scale. Nevertheless, the analysis of this data using neural network architectures lags behind the current state-of-the-art on popular datasets such as {PASCAL} {VOC}, {CityScapes} and Camvid. In this paper we bridge the performance-gap between these popular datasets and aerial imagery data. Little work is done on aerial imagery with state-of-the-art neural network architectures in a multi-class setting. Our experiments concerning data augmentation, normalisation, image size and loss functions give insight into a high performance setup for aerial imagery segmentation datasets. Our work, using the state-of-the-art {DeepLabv}3+ Xception65 architecture, achieves a mean {IOU} of 70\% on the {DroneDeploy} validation set. With this result, we clearly outperform the current publicly available state-of-the-art validation set {mIOU} (65\%) performance with 5\%. Furthermore, to our knowledge, there is no {mIOU} benchmark for the test set. Hence, we also propose a new benchmark on the {DroneDeploy} test set using the best performing {DeepLabv}3+ Xception65 architecture, with a {mIOU} score of 52.5\%.},
	number = {{arXiv}:2012.02024},
	publisher = {{arXiv}},
	author = {Heffels, Michael R. and Vanschoren, Joaquin},
	urldate = {2023-08-25},
	date = {2020-12-03},
	eprinttype = {arxiv},
	eprint = {2012.02024 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.6},
	file = {arXiv Fulltext PDF:/home/hugo/Zotero/storage/J8U4RKHM/Heffels and Vanschoren - 2020 - Aerial Imagery Pixel-level Segmentation.pdf:application/pdf;arXiv.org Snapshot:/home/hugo/Zotero/storage/MRY77AWT/2012.html:text/html},
}

@thesis{madje_programmable_2022,
	title = {A Programmable Markup Language for Typesetting},
	url = {https://www.user.tu-berlin.de/laurmaedje/programmable-markup-language-for-typesetting.pdf},
	institution = {tu-berlin},
	type = {phdthesis},
	author = {Mädje, Laurenz},
	date = {2022},
}
